import requests
import gzip
import os
import re

URL_REGEX = r'https?:\/\/(www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()@:%_\+.~#?&//=]*)'
HOST_REGEX = r'[^:\/\s]+'


def get_notifications(api_key, scan_id):
    headers = {
        'accept': 'application/json, text/plain, */*',
        'accept-encoding': 'gzip, deflate, br',
        'accept-language': 'en-US,en;q=0.9',
        'Authorization': f'api-key {api_key}',
    }

    response = requests.get(
        f'https://app.neuralegion.com/api/v1/scans/{scan_id}/logs/archive', headers=headers)
    with open(f"log-{scan_id}.gz", 'wb') as f:
        for chunk in response.iter_content(chunk_size=1024):
            if chunk:
                f.write(chunk)
                f.flush()

    with open(f"log-{scan_id}.txt", 'wb') as txtf:
        with gzip.open(f'log-{scan_id}.gz', 'rb') as zipf:
            file_content = zipf.read()
        txtf.write(file_content)
        txtf.flush()
        os.remove(f"log-{scan_id}.gz")


def analyze_file(path):
    counters = {'skipped': 0, 'added': 0, 'long_response': 0,
                'ssl_error': 0, '404_response': 0, 'network_error': 0}
    hosts = {}
    unaccounted_lines = ""

    def update_hosts(raw_line):
        url = re.search(URL_REGEX, raw_line).group()
        host = re.search(HOST_REGEX, url.removeprefix(
            "https" if url.startswith("https") else "http")).group()
        if host in hosts.keys():
            hosts[host] += 1
        else:
            hosts[host] = 0

    with open(path, 'r', encoding='utf-8') as file:
        for line in file:
            if 'Skipped url because of ‘ignored path’ regex' in line:
                counters['skipped'] += 1
                update_hosts(line)
            elif 'Added a new entry point' in line:
                counters['added'] += 1
                update_hosts(line)
            elif 'due to response time' in line:
                counters['long_response'] += 1
                update_hosts(line)
            elif 'OpenSSL::SSL::Error' in line:
                counters['ssl_error'] += 1
                update_hosts(line)
            elif 'response status being 404' in line:
                counters['404_response'] += 1
                update_hosts(line)
            elif '3 network errors' in line:
                counters['network_error'] += 1
                update_hosts(line)
            else:
                unaccounted_lines += f'{line}\n'
    os.remove("log.txt")
    return(counters, hosts, unaccounted_lines)


def analyze(api_key, scan_id):
    get_notifications(api_key, scan_id)
    data = analyze_file(f'log-{scan_id}.txt')
    print(data[2])
    ep_info = f"added {data[0]['added']} EPs, skipped {data[0]['skipped']}, long response time: {data[0]['long_response']}, ssl error: {data[0]['ssl_error']}, 404 responses: {data[0]['404_response']}, 3 network errors in a row: {data[0]['network_error']}"
    hosts_info = f"total hosts: {len(list(data[1].keys()))} - {dict(sorted(data[1].items(), key=lambda item: item[1], reverse=True))}"
    return(f'EPs: {ep_info}\n\nHosts: {hosts_info}')
