import requests
import gzip
import os
import re
import time

URL_REGEX = r'https?:\/\/(www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()@:%_\+.~#?&//=]*)'
HOST_REGEX = r'[^:\/\s]+'


def get_notifications(api_key, scan_id, timestamp):
    try:
        headers = {
            'accept': 'application/json, text/plain, */*',
            'accept-encoding': 'gzip, deflate, br',
            'accept-language': 'en-US,en;q=0.9',
            'Authorization': f'api-key {api_key}',
        }

        zip_name = f"log-{scan_id}-{timestamp}.gz"
        txt_name = f"log-{scan_id}-{timestamp}.txt"

        response = requests.get(
            f'https://app.neuralegion.com/api/v1/scans/{scan_id}/logs/archive', headers=headers)
        with open(zip_name, 'wb') as f:
            for chunk in response.iter_content(chunk_size=1024):
                if chunk:
                    f.write(chunk)
                    f.flush()
        with open(txt_name, 'wb') as txtf:
            with gzip.open(zip_name, 'rb') as zipf:
                file_content = zipf.read()
            txtf.write(file_content)
            txtf.flush()
    except:
        if os.path.exists(txt_name):
            os.remove(txt_name)
        return("Failed to get engine notifications")
    finally:
        if os.path.exists(zip_name):
            os.remove(zip_name)


def analyze_file(txt_file):
    try:
        counters = {'skipped': 0, 'added': 0, 'long response': 0,
                    'ssl error': 0, '404 response': 0, 'network error': 0}
        hosts = {}
        unaccounted_lines = ""

        def update_hosts(raw_line):
            url = re.search(URL_REGEX, raw_line).group()
            host = re.search(HOST_REGEX, url.removeprefix(
                "https" if url.startswith("https") else "http")).group()
            if host in hosts.keys():
                hosts[host] += 1
            else:
                hosts[host] = 0

        with open(txt_file, 'r', encoding='utf-8') as file:
            for line in file:
                if 'Skipped url because of ‘ignored path’ regex' in line:
                    counters['skipped'] += 1
                    update_hosts(line)
                elif 'Added a new entry point' in line:
                    counters['added'] += 1
                    update_hosts(line)
                elif 'due to response time' in line:
                    counters['long response'] += 1
                    update_hosts(line)
                elif 'OpenSSL::SSL::Error' in line:
                    counters['ssl error'] += 1
                    update_hosts(line)
                elif 'response status being 404' in line:
                    counters['404 response'] += 1
                    update_hosts(line)
                elif '3 network errors' in line:
                    counters['network error'] += 1
                    update_hosts(line)
                else:
                    unaccounted_lines += f'{line}\n'
        counters["total"]=sum(counters.values())
        return(counters, hosts, unaccounted_lines)
    except:
        return("Failed to analyze file")
    finally:
        if os.path.exists(txt_file):
            os.remove(txt_file)


def analyze(api_key, scan_id):
    timestamp = time.time()
    get_log_error = get_notifications(api_key, scan_id, timestamp)
    if get_log_error == None:
        data = analyze_file(f'log-{scan_id}-{timestamp}.txt')
        if type(data) == str:
            return data
        eps = data[0]
        hosts = dict(
            sorted(data[1].items(), key=lambda item: item[1], reverse=True))
        return(eps, hosts)
    else:
        return get_log_error
